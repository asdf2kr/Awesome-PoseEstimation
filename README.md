# Awesome-Pose Estimation [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/asdf2kr/Awesome-PoseEstimation)
<br>
This is a collection of papers for pose estimation.
<br>

# Table of Contents
- [Table of Contents](#Table-of-Contents)
- [Papers and Code](#Papers-and-Code)
  - [Top-Down Approach](#Top-Down-Approach)
  - [Bottom-Up Approach](#Bottom-Up-Approach)
  - [End-To-End Approach](#End-To-End-Approach)
- [Datasets](#Datasets)
---

# Papers
## Top-Down Approach
<font size=3><b>RMPE: Regional Multi-person Pose Estimation. (AlphaPose)</b></font>
<br> <font size=2>Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, Cewu Lu </font>
<br> <font size=2>ICCV2017 [[paper]](https://arxiv.org/abs/1612.00137) [[code]](https://github.com/MVIG-SJTU/RMPE)</font>

<br> <font size=3><b>Deep High-Resolution Representation Learning for Human Pose Estimation. </b></font>
<br> <font size=2>Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang </font>
<br> <font size=2>CVPR2019 [[paper]](https://arxiv.org/abs/1902.09212) [[code]](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=heatmap&color=blue"/>

<br> <font size=3><b>Pose Recognition with Cascade Transformers. </b></font>
<br> <font size=2>Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, Zhuowen Tu </font>
<br> <font size=2>CVPR2021 [[paper]](https://arxiv.org/abs/2104.06976) [[code]](https://github.com/mlpc-ucsd/PRTR)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=regression&color=green"/>

<br> <font size=3><b>TransPose: Keypoint Localization via Transformer. </b></font>
<br> <font size=2>Sen Yang, Zhibin Quan, Mu Nie, Wankou Yang </font>
<br> <font size=2>ICCV2021 [[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_TransPose_Keypoint_Localization_via_Transformer_ICCV_2021_paper.pdf) [[code]](https://github.com/yangsenius/TransPose)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=heatmap&color=blue"/>

<br> <font size=3><b>TokenPose: Learning Keypoint Tokens for Human Pose Estimation. </b></font>
<br> <font size=2>Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou Yang, Shu-Tao Xia, Erjin Zhou </font>
<br> <font size=2>ICCV2021 [[paper]](https://arxiv.org/pdf/2104.03516) [[code]]()</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=heatmap&color=blue"/>

<br> <font size=3><b>TFPose: Direct Human Pose Estimation with Transformers. </b></font>
<br> <font size=2>Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang </font>
<br> <font size=2>arXiv2021 [[paper]](https://arxiv.org/abs/2103.15320) [[code]](https://github.com/aim-uofa/AdelaiDet/)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=regression&color=green"/>

<br> <font size=3><b>HRFormer: High-Resolution Transformer for Dense Prediction. </b></font>
<br> <font size=2>Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, Jingdong,Wang </font>
<br> <font size=2>NIPS2021 [[paper]](https://arxiv.org/abs/2110.09408) [[code]](https://github.com/HRNet/HRFormer)</font>

<br> <font size=3><b>ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. </b></font>
<br> <font size=2>Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao </font>
<br> <font size=2>arXiv2022 [[paper]]() [[code]](https://github.com/vitae-transformer/vitpose)</font>

## Bottom-Up Approach
<br> <font size=3><b>DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation. </b></font>
<br> <font size=2>Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler , Bernt Schiele </font>
<br> <font size=2>CVPR2016 [[paper]](https://arxiv.org/abs/1511.06645) [[code]](https://github.com/eldar/deepcut)</font>

<br> <font size=3><b>DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model. </b></font>
<br> <font size=2>Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler and Bernt Schiele </font>
<br> <font size=2>ECCV2016 [[paper]](https://arxiv.org/abs/1605.03170) [[code]](https://github.com/eldar/deepcut-cnn)</font>

<br> <font size=3><b>Openpose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. </b></font>
<br> <font size=2>Zhe Cao Tomas Simon Shih-En Wei Yaser Sheikh </font>
<br> <font size=2>CVPR2017 [[paper]](https://arxiv.org/abs/1812.08008) [[code]](https://github.com/CMU-Perceptual-Computing-Lab/openpose)</font>

<br> <font size=3><b>Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression. </b></font>
<br> <font size=2>Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang </font>
<br> <font size=2>CVPR2021 [[paper]](https://arxiv.org/abs/2104.02300) [[code]](https://github.com/HRNet/DEKR)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=heatmap&color=blue"/>

## End-To-End Approach
<br> <font size=3><b>End-to-End Trainable Multi-Instance Pose Estimation with Transformers </b></font>
<br> <font size=2>Lucas Stoffl, Maxime Vidal, Alexander Mathis </font>
<br> <font size=2>arXiv2021 [[paper]](https://arxiv.org/abs/2103.12115) [[code]]()</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=heatmap&color=blue"/>

<br> <font size=3><b>End-to-End Multi-Person Pose Estimation with Transformers. </b></font>
<br> <font size=2>Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, Wenming Tan </font>
<br> <font size=2>CVPR2022 [[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.pdf) [[code]](https://github.com/hikvision-research/opera)</font> <img alt="Html" src ="https://img.shields.io/static/v1?label=&message=regression&color=green"/>


---
# Datasets
<br> <font size=3><b>COCO Datasets<b> </b></font>
<font size=2>[[paper]](https://arxiv.org/abs/1405.0312) [[website]](https://cocodataset.org/#home)</font>

<br> <font size=3><b>MPII Human Pose Dataset<b> </b></font>
<font size=2>[[paper]](http://human-pose.mpi-inf.mpg.de/contents/andriluka14cvpr.pdf) [[website]](http://human-pose.mpi-inf.mpg.de/)</font>

<br> <font size=3><b>LSP (Leeds Sports Pose) Dataset<b> </b></font>
<font size=2>[[paper]]() [[website]](http://sam.johnson.io/research/lsp.html)</font>

<br> <font size=3><b>FLIC (Frames labeled In Cinema) Dataset<b> </b></font>
<font size=2>[[paper]]() [[website]](https://bensapp.github.io/flic-dataset.html)</font>

<br> <font size=3><b>PoseTrack Datasets<b> </b></font>
<font size=2>[[paper]]() [[website]](https://posetrack.net)</font>
